{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "couldn't import doomish\n",
      "Couldn't import doom\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import clone_model\n",
    "from keras.callbacks import TensorBoard\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym_ple  # noqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    # 入力する画像サイズ（縦*横*フレーム）\n",
    "    INPUT_SHAPE = (80, 80, 4)\n",
    "    # 初期化\n",
    "    def __init__(self, num_actions):\n",
    "        # 行動の数\n",
    "        ## 今回は3で固定だが、他のゲームにも対応できるような作りになっている\n",
    "        self.num_actions = num_actions\n",
    "        # モデルの定義\n",
    "        ## 3つの畳み込み層->2つの全結合層\n",
    "        ## activationとしてはreluを使用\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(\n",
    "            32, kernel_size=8, strides=4, padding=\"same\",\n",
    "            input_shape=self.INPUT_SHAPE, kernel_initializer=\"normal\",\n",
    "            activation=\"relu\"))\n",
    "        model.add(Conv2D(\n",
    "            64, kernel_size=4, strides=2, padding=\"same\",\n",
    "            kernel_initializer=\"normal\",\n",
    "            activation=\"relu\"))\n",
    "        model.add(Conv2D(\n",
    "            64, kernel_size=3, strides=1, padding=\"same\",\n",
    "            kernel_initializer=\"normal\",\n",
    "            activation=\"relu\"))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, kernel_initializer=\"normal\", activation=\"relu\"))\n",
    "        model.add(Dense(num_actions, kernel_initializer=\"normal\"))\n",
    "        self.model = model\n",
    "    # 評価\n",
    "    def evaluate(self, state, model=None):\n",
    "        _model = model if model else self.model\n",
    "        _state = np.expand_dims(state, axis=0)  # add batch size dimension\n",
    "        # 実際にモデルで評価値を予測させる\n",
    "        return _model.predict(_state)[0]\n",
    "    # 行動\n",
    "    def act(self, state, epsilon=0):\n",
    "        # epsilon-greedy法\n",
    "        ## epsilonより小さいならランダムに行動する\n",
    "        if np.random.rand() <= epsilon:\n",
    "            a = np.random.randint(low=0, high=self.num_actions, size=1)[0]\n",
    "        else:\n",
    "            q = self.evaluate(state)\n",
    "            # 評価値が最大の行動を取得\n",
    "            a = np.argmax(q)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 観測\n",
    "class Observer(object):\n",
    "\n",
    "    def __init__(self, input_shape):\n",
    "        self.size = input_shape[:2]  # width x height\n",
    "        self.num_frames = input_shape[2]  # number of frames\n",
    "        self._frames = []\n",
    "\n",
    "    def observe(self, state):\n",
    "        # グレースケール化\n",
    "        g_state = Image.fromarray(state).convert(\"L\")  # to gray scale\n",
    "        # 想定しているsize(今回は80*80)に加工\n",
    "        g_state = g_state.resize(self.size)  # resize game screen to input size\n",
    "        g_state = np.array(g_state).astype(\"float\")\n",
    "        g_state /= 255  # scale to 0~1\n",
    "        # もし最初の画面だった場合は4つに複製させる\n",
    "        if len(self._frames) == 0:\n",
    "            # full fill the frame cache\n",
    "            self._frames = [g_state] * self.num_frames\n",
    "        else:\n",
    "            # 最新の画面を追加し、古い画面を出す処理\n",
    "            self._frames.append(g_state)\n",
    "            self._frames.pop(0)  # remove most old state\n",
    "\n",
    "        input_state = np.array(self._frames)\n",
    "        # change frame_num x width x height => width x height x frame_num\n",
    "        input_state = np.transpose(input_state, (1, 2, 0))\n",
    "        return input_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エージェントを学習させる処理\n",
    "class Trainer(object):\n",
    "\n",
    "    def __init__(self, env, agent, optimizer, model_dir=\"\"):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "        # Experience Replayに必要\n",
    "        # 経験を蓄積させていく\n",
    "        self.experience = []\n",
    "        # 一定期間重みが固定されたモデルでQ値を出力したいので利用する\n",
    "        self._target_model = clone_model(self.agent.model)\n",
    "        self.observer = Observer(agent.INPUT_SHAPE)\n",
    "        self.model_dir = model_dir\n",
    "        if not self.model_dir:\n",
    "            #self.model_dir = os.path.join(os.path.dirname(__file__), \"model\")\n",
    "            self.model_dir = os.path.join(os.path.abspath(\"__file__\"), \"model\")\n",
    "            if not os.path.isdir(self.model_dir):\n",
    "                os.mkdir(self.model_dir)\n",
    "        # mseで最適化\n",
    "        self.agent.model.compile(optimizer=optimizer, loss=\"mse\")\n",
    "        self.callback = TensorBoard(self.model_dir)\n",
    "        self.callback.set_model(self.agent.model)\n",
    "    \n",
    "    # バッチの取得をする\n",
    "    def get_batch(self, batch_size, gamma):\n",
    "        # self.experienceからランダムにバッチ数だけデータを取得する\n",
    "        batch_indices = np.random.randint(\n",
    "            low=0, high=len(self.experience), size=batch_size)\n",
    "        X = np.zeros((batch_size,) + self.agent.INPUT_SHAPE)\n",
    "        y = np.zeros((batch_size, self.agent.num_actions))\n",
    "        for i, b_i in enumerate(batch_indices):\n",
    "            s, a, r, next_s, game_over = self.experience[b_i]\n",
    "            X[i] = s\n",
    "            y[i] = self.agent.evaluate(s)\n",
    "            # future reward\n",
    "            # 次の時点での将来的に得られる最大値を取得\n",
    "            # 一定時間重みを固定したモデルを計算に利用する\n",
    "            Q_sa = np.max(self.agent.evaluate(next_s,\n",
    "                                              model=self._target_model))\n",
    "            # ベルマン方程式\n",
    "            # Q(s_t, a_t) = r + gamma*Q(s_t+1, a_t+1)\n",
    "            if game_over:\n",
    "                y[i, a] = r\n",
    "            else:\n",
    "                y[i, a] = r + gamma * Q_sa\n",
    "        return X, y\n",
    "\n",
    "    def write_log(self, index, loss, score):\n",
    "        for name, value in zip((\"loss\", \"score\"), (loss, score)):\n",
    "            summary = tf.Summary()\n",
    "            summary_value = summary.value.add()\n",
    "            summary_value.simple_value = value\n",
    "            summary_value.tag = name\n",
    "            self.callback.writer.add_summary(summary, index)\n",
    "            self.callback.writer.flush()\n",
    "    \n",
    "    def train(self,\n",
    "              gamma=0.99,\n",
    "              initial_epsilon=0.1, final_epsilon=0.0001,\n",
    "              memory_size=50000,\n",
    "              observation_epochs=100, training_epochs=2000,\n",
    "              batch_size=32, render=True):\n",
    "        self.experience = deque(maxlen=memory_size)\n",
    "        # 最初に100回観測したあと、2000回学習させるイメージ\n",
    "        epochs = observation_epochs + training_epochs\n",
    "        epsilon = initial_epsilon\n",
    "        model_path = os.path.join(self.model_dir, \"agent_network.h5\")\n",
    "        fmt = \"Epoch {:04d}/{:d} | Loss {:.5f} | Score: {} | e={:.4f} train={}\"\n",
    "        #学習ループ\n",
    "        for e in range(epochs):\n",
    "            loss = 0.0\n",
    "            rewards = []\n",
    "            # 環境を初期化させる（スタート画面に戻すような感じ）\n",
    "            initial_state = self.env.reset()\n",
    "            state = self.observer.observe(initial_state)\n",
    "            game_over = False\n",
    "            is_training = True if e > observation_epochs else False\n",
    "\n",
    "            # let's play the game\n",
    "            while not game_over:\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "                # 学習してないときは完全にランダムに動かす\n",
    "                if not is_training:\n",
    "                    action = self.agent.act(state, epsilon=1)\n",
    "                else:\n",
    "                    action = self.agent.act(state, epsilon)\n",
    "                # 次の状態、報酬、ゲームオーバーになったか、を取得\n",
    "                next_state, reward, game_over, info = self.env.step(action)\n",
    "                next_state = self.observer.observe(next_state)\n",
    "                # 行動、報酬、次の状態といった一連の情報をexperienceに追加していく\n",
    "                self.experience.append(\n",
    "                    (state, action, reward, next_state, game_over)\n",
    "                    )\n",
    "\n",
    "                rewards.append(reward)\n",
    "            \n",
    "                if is_training:\n",
    "                    # バッチ作成、学習\n",
    "                    X, y = self.get_batch(batch_size, gamma)\n",
    "                    loss += self.agent.model.train_on_batch(X, y)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "            loss = loss / len(rewards)\n",
    "            score = sum(rewards)\n",
    "\n",
    "            if is_training:\n",
    "                self.write_log(e - observation_epochs, loss, score)\n",
    "                # 重み更新\n",
    "                self._target_model.set_weights(self.agent.model.get_weights())\n",
    "            # epsilonを徐々に小さくしていく\n",
    "            if epsilon > final_epsilon:\n",
    "                epsilon -= (initial_epsilon - final_epsilon) / epochs\n",
    "\n",
    "            print(fmt.format(e + 1, epochs, loss, score, epsilon, is_training))\n",
    "\n",
    "            if e % 100 == 0:\n",
    "                self.agent.model.save(model_path, overwrite=True)\n",
    "\n",
    "        self.agent.model.save(model_path, overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(render):\n",
    "    env = gym.make(\"Catcher-v0\")\n",
    "    num_actions = env.action_space.n\n",
    "    agent = Agent(num_actions)\n",
    "    trainer = Trainer(env, agent, Adam(lr=1e-6))\n",
    "    trainer.train(render=render)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 指定されたパスが見つかりません。: 'C:\\\\Users\\\\hmasa\\\\study\\\\deep-learning-with-keras\\\\chapter8\\\\8-2_8-3\\\\__file__\\\\model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-715885ac5995>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-170bf3acdf60>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(render)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mnum_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtrainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-88a78245fd14>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, env, agent, optimizer, model_dir)\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"__file__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;31m# mseで最適化\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"mse\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 指定されたパスが見つかりません。: 'C:\\\\Users\\\\hmasa\\\\study\\\\deep-learning-with-keras\\\\chapter8\\\\8-2_8-3\\\\__file__\\\\model'"
     ]
    }
   ],
   "source": [
    "main(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
