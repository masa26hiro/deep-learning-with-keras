## 8.2 ボールキャッチゲームのためのDeep Q-networkの実装
* この節では、実際にDeep Q-networkを使ってボールキャッチゲームを学習させてみる。
* 詳細はコード参照
* DeepMindがDeep Q-networkを発表してから改善された手法
    * Double Q-learning
        * 2つのネットワークを利用する方法で、行動の選択とその評価を別々に行う 
    * Prioritized Experience Replay
        * 蓄積した経験の中から、学習に役立つものを優先的に抽出する手法
    * Dueling Network Architectures
        * Q関数を状態の評価と行動の評価に分解し、別個に価値評価を学習させる

## 8.3 強化学習を取り巻く状況
* AlphaGOについて
    * 人間の騎士道氏の対局データを利用した教師あり学習と、自分自身のコピー同士の対局データを利用した強化学習を組み合わせた
    * 価値関数と方策関数を組み合わせている
        * 次の手を打つ時にどの手を打ったら勝率が高くなるかをモンテカルロ法で計算している。これに価値関数を利用している
        * 最後まで探索するかどうかの判断の足切りのようなイメージ
        * 方策関数によって探索を行うべき手を減らし、望みが高いもののみ計算する
