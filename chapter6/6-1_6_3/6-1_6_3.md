# 6章　リカレントニューラルネットワーク

## 導入
- RNN:入力間の依存性を利用するNNの一種
- 音声認識、言語モデリング、機械翻訳、評判分析、画像キャプションなどに利用
- 本章で触れるRNN
  - 単純なRNN (SimpleRNN)
  - LSTM (long hosrt-term memory)
  - GRU (gated recurrent unit)

## 6.1 SimpleRNNセル
  - 過去に出現したデータのエッセンスを保持する隠れ状態を持つことで依存性を考慮する

  <img src="https://latex.codecogs.com/gif.latex?h_t&space;=&space;\phi(h_{t-1},&space;x_t)" />
  - RNNのパラメータは入力、出力、隠れ状態に対応する3つの重み行列U,V,Wに含まれる
  - 重み行列U,V,Wはステップ間で共有されているためRNNが学習するパラメータ数は大幅に削減される
  <img src="https://latex.codecogs.com/gif.latex?h_t&space;&=&&space;\tanh(Wh_{t-1}&plus;Ux_t)&space;\\&space;y_t&space;&=&&space;\mathrm{softmax}(Vh_t)" />

### 6.1.1 RNNを用いたテキスト作成
  - 自然言語処理分野でRNNは広く用いられる
  - 応用例の一つに言語モデルの構築
    - 言語モデルを使用することで、単語をいくつか与えたときに次に出現する単語の確率を予測できる
    - 機械翻訳、スペル訂正など様々な上位レベルのタスクにとって重要な技術
  - 生成モデル:出力確率をサンプリングしてテキストを生成できるモデル
  - 言語モデル:入力が単語列で出りょっくは予測された単語列
  - 例）「不思議の国のアリス」を使って文字ベースの言語モデルを学習する
  - この種のモデルでは、株価予測、クラシック音楽の生成などに成功している

## 6.2 RNNのトポロジー
  - MLP, CNNのアーキテクチャはどちらも固定サイズのテンソルを入力として受け取り、固定サイズのテンソルを出力として生成する
  - 入力から出力への返還をモデル内の階層によって与えられる固定のステップ数で実行する
  - RNNには上記のような制約がない
    - 入力、出力、またはその両方に系列を与えることができる→様々な方法で構築ができる
  - RNNは本質的にコンピュータプログラムを記述するものと考えられる(...?)
    - RNNはチューリング完全　らしい
  - 系列を扱うことができる特性により多くのトポロジーを生み出せる
    - (a) 一対一
      - 他トポロジーの派生元
      - すべての入力系列が同じ長さで各時刻で出力が生成される
    - (b) 多対多
      - 機械翻訳でよく使われ、Seq2seqとも呼ばれる
      - 系列を入力すると別の系列を生成する
      - ある時刻では入力がなく、ある時刻では出力がない
    - (c) 一対多
      - 画像キャプションなどで使われる
    - (d) 多対一
      - 評判分析でよく使われる

## 6.3　勾配消失と勾配爆発
  - RNNでも誤差逆伝搬法により学習を行う
  - 通常のNNと違い、パラメータがすべてのタイムステップで共有されている
    - 各出力の勾配は現在だけでなく過去のタイムステップにも依存
  - このプロセスをBPTTと呼ぶ
  - 勾配消失の影響は離れたステップからの勾配が学習になにも貢献しない形で現れる
    - 従来のNNよりもRNNの方が勾配消失が顕著にあらわれる
  - 勾配爆発は勾配が非常に大きくなり、学習が破綻する
  - 勾配消失問題を最小化するために以下のような方法が提案されている
    - 行列Wの初期値を適切に設定する
    - 活性化関数にReLUを使う
    - 教師なしで各層を事前学習する
  - 最も一般的な方法はLSTMやGRUを使用すること
