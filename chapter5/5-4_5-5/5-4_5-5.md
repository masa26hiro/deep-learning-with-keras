# 5.4 事前学習済みベクトルの使用

分散表現を自身のネットワークで使用する方法は以下の3パターンがある

- ゼロから分散表現を学習する
    - 分散表現の重みは小さなランダム値に初期化され、誤差逆伝搬法で学習する
    - 前節でのSkip-gram、CBOWはこの方法を使用

- 事前学習済み GloVe/word2vec モデルをファインチューニングする
    - 事前学習済みモデルから重み行列を作成し、誤差逆伝搬法で学習する
    - 学習済みのモデルから重みを生成するため、早く収束する

- 事前学習済み GloVe/woed2vec モデルから分散表現を検索する
    - 事前学習済みモデルから単語分散表現を検索し、入力を分散表現に変換する。
      その後、変換された入力を用いて機械学習モデルを学習する
    - 学習するモデルはニューラルネットワークである必要はない
    - 事前学習済みモデルがターゲットドメインと同じドメインで学習されている場合、
      コストを抑えて動作できる


## ゼロから分散表現を学習する

### CNNを利用した分のポジティブ／ネガティブ分類

#### 単語 n-gram
- NLP(Neuro Linguistic Programing 神経言語プログラミング)への伝統的なアプローチ
- 任意の一文をn個の単語ごとに分割する手法のこと
- 例「今日はいい天気ですね。」を1-gram(unigram)、2-gram(bigram)、3-gram(trigram)するとき
    - unigram

        `'今日', 'は', 'いい', '天気', 'です', 'ね', '。'`

    - bigram

        `'今日は', 'はいい', 'いい天気', '天気です', 'ですね', 'ね。'`

    - trigram

        `'今日はいい', 'はいい天気', 'いい天気です', '天気ですね', 'ですね。'`

- 1次元のCNNでは1度に数単語を処理する畳み込みフィルターを使用して学習し、
  その結果をぷ０リングしてもっとも重要な概念を表現するベクトルを作成する

### RNNを利用した学習
- 詳細は次章
- 入力テキストは単語IDの系列に変換される
- 文や単語への変換はNLTK(Natural Language Toolkit)を用いる
