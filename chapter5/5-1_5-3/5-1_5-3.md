# 単語分散表現
　単語やフレーズをベクトルとして表現する方法。これによって、テキストデータの解析や機械学習アルゴリズムの適用が可能となる。

一番基礎的な手法として、one-hot encodingがあげられる。
* one-hot encoding
    * ある単語を、語彙数と同じベクトルで表現。その単語であるか否かのバイナリで表現する。そのため、高次元ベクトルでかつスパースな表現となってしまう。また、単語間の類似度の計算を行うことができない。

これらの問題を解決する手法として、TFIDF, 潜在意味解析、トピックモデルといった手法が提案された。しかしながら、これらの手法では単語の意味そのものを表現するものではない。
* TFIDF
    * 文書をベクトルで表現する手法で、語彙数と同じベクトルで表現する。
    * TF(term frequency)は各単語が文書内でどのくらい出現したかで重みづけする。
    * IDF(inverse document frequency)はその単語が他の文書で出現しないようなレア単語なら高い値を、よく出現する単語なら低い値を重みづけする手法
* 潜在意味解析（LSA）
    * TFIDFで作成した単語文書行列を特異値分解することで次元圧縮して表現する手法。
    * 文書間、単語間の類似度を表現することが可能
    * [【技術解説】潜在意味解析(LSA) ～特異値分解(SVD)から文書検索まで～](https://mieruca-ai.com/ai/lsa-lsi-svd/)
* トピックモデル(LDA)
    * [参考](https://www.albert2005.co.jp/knowledge/machine_learning/topic_model/about_topic_model)

    ![トピックモデル](https://www.albert2005.co.jp/knowledge/images/tech_machine_learning_img03.jpg)

この章では、単語分散表現として近年知られている**word2vec**, **GloVe** という手法について解説する。

## 分散表現とは
　単語の意味とは、その単語の周辺にある単語（文脈）との関係によって決まると考える。

### 例文
```
Paris is the capital of France
Berlin is the capital of Germany
```
この文章で、ParisとBerlinのペアと、FranceとGermanyのペアの関係が似ていると考えることができる。

単語分散表現では、以下のような関係を表現できることを目指す
```
Paris - France = Berlin - Germany
```

## Word2Vec
このモデルは教師なし学習であり、これまでのように正解ラベルは不要。Word2Vecは手法というよりもツールの名称と考えたほうがいい。

大規模なテキストコーパスを入力すると、各単語のベクトル表現を生成する。

Word2Vecを実現するアーキテクチャとして2つあげられる。
* CBOW(continuous bag-of-words)
    * ある単語の周辺単語（文脈語）からその中心語を予測するモデル
* Skip-gram
    * 中心語を入力して、その周辺の単語（文脈語）を予測するモデル

## Skip-gram
　ある単語を与えられた時、その文脈語を予測するように学習する。

例文
``` 
I love green eggs and ham.
```
ウィンドウサイズを1としたとき、文脈語、中心語のペアは以下のようになる。
```
([I, green], love)
([love, eggs], green)
([green, and], eggs)
```
Skip-gramでは、このペアを正例として学習をさせる。
正解のペア(中心語、文脈語)以下のようなイメージ。
```
(love, I), (love, green), (green, eggs),...
```
負例については、各入力語と、ランダムな単語で生成する。

単語は、以降idで表現する。（idと単語の辞書をあらかじめ作っておく）

詳細はgoogle slidesに記述

https://docs.google.com/presentation/d/14K6D_she3Oyk_8BxGAlvmf8aEHJbXDUPSCizL5hSdc0/edit?usp=sharing
* Embedding層
    * 単語のidをある固定次元のベクトルに変換する層
    * [What the heck is Word Embedding](https://towardsdatascience.com/what-the-heck-is-word-embedding-b30f67f01c81)

## CBOW
　文脈語から、中心語を予測するように学習する。
```
([I, green], love)
([love, eggs], green)
([green, and], eggs)
```
この例だと、文脈語I, green から、loveを予測する。

## 分散表現の抽出
　Skip-gram、CBOWの学習の過程で得ることができるEmbedding層を用いて分散表現を抽出する。

モデルの学習過程において、入力語から出力を予測するのに十分な情報を獲得することができるため、抽出することができる。

## GloVe
　単語表現のグローバルベクトル。学習はコーパス全体の単語共起統計量を元に行われる。word2vecは予測ベースのモデルであり、カウントベースのモデルである。

　まず、巨大なコーパスにおける単語、文脈ペアを表現する共起行列を作成。作成した単語文脈共起行列に対して、行列分解（matrix factorization）を実行する。これにより、2つの行列の積によって単語文脈行列を表現できるようになる。

まず最初に2つの行列はランダムな値に設定され、確率的勾配降下法によって値を更新していく。
```
R = P * Q = R'
```
RとR'の誤差を計算し、PとQをどれだけ変化させれば、R' がRに近づくかを計算して更新していく。

