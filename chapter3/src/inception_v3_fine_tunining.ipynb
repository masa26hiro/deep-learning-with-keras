{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import SGD\n",
    "import os\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hmasa\\Anaconda3\\envs\\tf-keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hmasa\\Anaconda3\\envs\\tf-keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4074: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_model = InceptionV3(weights=\"imagenet\", include_top=False)# モデルロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "predictions = Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inceptionの部分は学習させない\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from keras.datasets import cifar10\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10Dataset():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.image_shape = (190, 190, 3)# inceptionのsizeに変更\n",
    "        self.num_classes = 10# クラス数\n",
    "        self.train_data_size = 5000\n",
    "        self.test_data_size = 5000\n",
    "    \n",
    "    def upscale(self, x, data_size):\n",
    "        data_upscaled = np.zeros((data_size,\n",
    "                                  self.image_shape[0],\n",
    "                                  self.image_shape[1],\n",
    "                                  self.image_shape[2]))\n",
    "        for i, img in enumerate(x):\n",
    "            large_img = cv2.resize(img, dsize=(self.image_shape[0],\n",
    "                                               self.image_shape[1]),)\n",
    "            data_upscaled[i] = large_img\n",
    "        return data_upscaled\n",
    "    \n",
    "    def get_batch(self):\n",
    "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()# ロード\n",
    "        # 画像を拡大\n",
    "        x_train = self.upscale(x_train, x_train.shape[0])\n",
    "        x_test = self.upscale(x_test, x_test.shape[0])\n",
    "        x_train = x_train[:self.train_data_size]\n",
    "        y_train = y_train[:self.train_data_size]\n",
    "        x_test = x_test[:self.test_data_size]\n",
    "        y_test = y_test[:self.test_data_size]\n",
    "        \n",
    "        x_train, x_test = [self.preprocess(d) for d in [x_train, x_test]]\n",
    "        y_train, y_test = [self.preprocess(d, label_data=True) for d in\n",
    "                           [y_train, y_test]]\n",
    "\n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "    def preprocess(self, data, label_data=False):\n",
    "        if label_data:\n",
    "            \n",
    "            data = keras.utils.to_categorical(data, self.num_classes)\n",
    "        else:# 正規化\n",
    "            data = data.astype(\"float32\")\n",
    "            data /= 255  \n",
    "            shape = (data.shape[0],) + self.image_shape  \n",
    "            data = data.reshape(shape)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "\n",
    "    def __init__(self, model, loss, optimizer):\n",
    "        self._target = model\n",
    "        self._target.compile(\n",
    "            loss=loss, optimizer=optimizer, metrics=[\"accuracy\"]\n",
    "            )\n",
    "        self.verbose = 1\n",
    "        logdir = \"logdir_\" + os.path.basename(\"__file__\").replace('.py', '')\n",
    "        self.log_dir = os.path.join(os.path.dirname(\"__file__\"), logdir)\n",
    "        self.model_file_name = \"model_file.hdf5\"\n",
    "\n",
    "    def train(self, x_train, y_train, batch_size, epochs, validation_split):\n",
    "        if os.path.exists(self.log_dir):\n",
    "            import shutil\n",
    "            shutil.rmtree(self.log_dir)\n",
    "        os.mkdir(self.log_dir)\n",
    "\n",
    "        datagen = ImageDataGenerator(\n",
    "            featurewise_center=False, \n",
    "            samplewise_center=False,\n",
    "            featurewise_std_normalization=False,  \n",
    "            samplewise_std_normalization=False, \n",
    "            zca_whitening=False,  \n",
    "            rotation_range=0, \n",
    "            width_shift_range=0.1,  \n",
    "            height_shift_range=0.1, \n",
    "            horizontal_flip=True, \n",
    "            vertical_flip=False) \n",
    "\n",
    "\n",
    "        datagen.fit(x_train)\n",
    "\n",
    "        indices = np.arange(x_train.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        validation_size = int(x_train.shape[0] * validation_split)\n",
    "        x_train, x_valid = \\\n",
    "            x_train[indices[:-validation_size], :], \\\n",
    "            x_train[indices[-validation_size:], :]\n",
    "        y_train, y_valid = \\\n",
    "            y_train[indices[:-validation_size], :], \\\n",
    "            y_train[indices[-validation_size:], :]\n",
    "\n",
    "        model_path = os.path.join(self.log_dir, self.model_file_name)\n",
    "        self._target.fit_generator(\n",
    "            datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "            steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=(x_valid, y_valid),\n",
    "            callbacks=[\n",
    "                TensorBoard(log_dir=self.log_dir),\n",
    "                ModelCheckpoint(model_path, save_best_only=True),\n",
    "                EarlyStopping(),\n",
    "            ],\n",
    "            verbose=self.verbose,\n",
    "            workers=4\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CIFAR10Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = dataset.get_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, loss=\"categorical_crossentropy\", optimizer=RMSprop())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hmasa\\Anaconda3\\envs\\tf-keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hmasa\\Anaconda3\\envs\\tf-keras\\lib\\site-packages\\keras\\callbacks\\tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hmasa\\Anaconda3\\envs\\tf-keras\\lib\\site-packages\\keras\\callbacks\\tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/8\n",
      "153/153 [==============================] - 12s 77ms/step - loss: 1.8090 - accuracy: 0.4540 - val_loss: 1.2672 - val_accuracy: 0.6520\n",
      "WARNING:tensorflow:From C:\\Users\\hmasa\\Anaconda3\\envs\\tf-keras\\lib\\site-packages\\keras\\callbacks\\tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "Epoch 2/8\n",
      "153/153 [==============================] - 6s 42ms/step - loss: 1.1427 - accuracy: 0.6122 - val_loss: 1.5570 - val_accuracy: 0.6540\n"
     ]
    }
   ],
   "source": [
    "# 数エポック学習\n",
    "trainer.train(\n",
    "    x_train, y_train, batch_size=26, epochs=8, validation_split=0.2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(os.path.join(trainer.log_dir, trainer.model_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 250層以降を学習できるようにする\n",
    "# inceptionの上位層部分のみを学習するイメージ\n",
    "for layer in model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, loss=\"categorical_crossentropy\",\n",
    "                  optimizer=SGD(lr=0.001, momentum=0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "153/153 [==============================] - 10s 64ms/step - loss: 0.8103 - accuracy: 0.7214 - val_loss: 1.0416 - val_accuracy: 0.7090\n",
      "Epoch 2/8\n",
      "153/153 [==============================] - 7s 44ms/step - loss: 0.6660 - accuracy: 0.7680 - val_loss: 0.9725 - val_accuracy: 0.7280\n",
      "Epoch 3/8\n",
      "153/153 [==============================] - 7s 45ms/step - loss: 0.5299 - accuracy: 0.8171 - val_loss: 0.9860 - val_accuracy: 0.7300\n"
     ]
    }
   ],
   "source": [
    "trainer.train(\n",
    "    x_train, y_train, batch_size=26, epochs=8, validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.9504465588569642\n",
      "Test accuracy: 0.7271999716758728\n"
     ]
    }
   ],
   "source": [
    "model = load_model(os.path.join(trainer.log_dir, trainer.model_file_name))\n",
    "# show result\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
