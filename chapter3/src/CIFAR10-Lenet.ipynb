{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\hmasa\\Anaconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\hmasa\\Anaconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\hmasa\\Anaconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\hmasa\\Anaconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\hmasa\\Anaconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\hmasa\\Anaconda3\\envs\\tf-keras\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\hmasa\\Anaconda3\\envs\\tf-keras\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\hmasa\\Anaconda3\\envs\\tf-keras\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\hmasa\\Anaconda3\\envs\\tf-keras\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\hmasa\\Anaconda3\\envs\\tf-keras\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\hmasa\\Anaconda3\\envs\\tf-keras\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\hmasa\\Anaconda3\\envs\\tf-keras\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten, Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.datasets import cifar10\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ネットワーク定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(\n",
    "        Conv2D(32, kernel_size=3, padding=\"same\",\n",
    "               input_shape=input_shape, activation=\"relu\")\n",
    "    )\n",
    "    # 32チャネル\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "    # 64チャネル\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセット定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10Dataset():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.image_shape = (32, 32, 3)# 入力sizeの定義\n",
    "        self.num_classes = 10# クラス数\n",
    "\n",
    "    def get_batch(self):\n",
    "        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "        x_train, x_test = [self.preprocess(d) for d in [x_train, x_test]]\n",
    "        y_train, y_test = [self.preprocess(d, label_data=True) for d in\n",
    "                           [y_train, y_test]]\n",
    "\n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "    def preprocess(self, data, label_data=False):\n",
    "        if label_data:\n",
    "            \n",
    "            data = keras.utils.to_categorical(data, self.num_classes)\n",
    "        else:# 正規化\n",
    "            data = data.astype(\"float32\")\n",
    "            data /= 255  \n",
    "            shape = (data.shape[0],) + self.image_shape  \n",
    "            data = data.reshape(shape)\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習処理定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "\n",
    "    def __init__(self, model, loss, optimizer):\n",
    "        self._target = model\n",
    "        # コンパイル\n",
    "        self._target.compile(\n",
    "            loss=loss, optimizer=optimizer, metrics=[\"accuracy\"]\n",
    "            )\n",
    "        self.verbose = 1\n",
    "        logdir = \"logdir_cifar10_net\"\n",
    "        self.log_dir = os.path.join(os.path.dirname(\"__file__\"), logdir)\n",
    "        self.model_file_name = \"model_file.hdf5\"\n",
    "\n",
    "    def train(self, x_train, y_train, batch_size, epochs, validation_split):\n",
    "        if os.path.exists(self.log_dir):\n",
    "            import shutil\n",
    "            shutil.rmtree(self.log_dir)\n",
    "        os.mkdir(self.log_dir)\n",
    "\n",
    "        model_path = os.path.join(self.log_dir, self.model_file_name)\n",
    "        self._target.fit(\n",
    "            x_train, y_train,\n",
    "            batch_size=batch_size, epochs=epochs,\n",
    "            validation_split=validation_split,\n",
    "            callbacks=[\n",
    "                TensorBoard(log_dir=self.log_dir),\n",
    "                ModelCheckpoint(model_path, save_best_only=True)\n",
    "            ],\n",
    "            verbose=self.verbose\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CIFAR10Dataset()\n",
    "model = network(dataset.image_shape, dataset.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hmasa\\Anaconda3\\envs\\tf-keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "WARNING:tensorflow:From C:\\Users\\hmasa\\Anaconda3\\envs\\tf-keras\\lib\\site-packages\\keras\\callbacks\\tensorboard_v1.py:200: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\hmasa\\Anaconda3\\envs\\tf-keras\\lib\\site-packages\\keras\\callbacks\\tensorboard_v1.py:203: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n",
      "Epoch 1/12\n",
      "40000/40000 [==============================] - 5s 135us/step - loss: 1.7531 - accuracy: 0.3715 - val_loss: 1.4629 - val_accuracy: 0.4962\n",
      "WARNING:tensorflow:From C:\\Users\\hmasa\\Anaconda3\\envs\\tf-keras\\lib\\site-packages\\keras\\callbacks\\tensorboard_v1.py:343: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "Epoch 2/12\n",
      "40000/40000 [==============================] - 2s 61us/step - loss: 1.3444 - accuracy: 0.5240 - val_loss: 1.2125 - val_accuracy: 0.5751\n",
      "Epoch 3/12\n",
      "40000/40000 [==============================] - 2s 61us/step - loss: 1.1739 - accuracy: 0.5848 - val_loss: 1.1138 - val_accuracy: 0.6059\n",
      "Epoch 4/12\n",
      "40000/40000 [==============================] - 3s 63us/step - loss: 1.0618 - accuracy: 0.6276 - val_loss: 1.0392 - val_accuracy: 0.6380\n",
      "Epoch 5/12\n",
      "40000/40000 [==============================] - 2s 62us/step - loss: 0.9778 - accuracy: 0.6561 - val_loss: 0.9695 - val_accuracy: 0.6582\n",
      "Epoch 6/12\n",
      "40000/40000 [==============================] - 2s 61us/step - loss: 0.9106 - accuracy: 0.6805 - val_loss: 0.9213 - val_accuracy: 0.6802\n",
      "Epoch 7/12\n",
      "40000/40000 [==============================] - 2s 61us/step - loss: 0.8548 - accuracy: 0.7010 - val_loss: 0.9472 - val_accuracy: 0.6714\n",
      "Epoch 8/12\n",
      "40000/40000 [==============================] - 2s 60us/step - loss: 0.8040 - accuracy: 0.7196 - val_loss: 1.0474 - val_accuracy: 0.6387\n",
      "Epoch 9/12\n",
      "40000/40000 [==============================] - 2s 60us/step - loss: 0.7604 - accuracy: 0.7359 - val_loss: 0.8799 - val_accuracy: 0.6992\n",
      "Epoch 10/12\n",
      "40000/40000 [==============================] - 2s 60us/step - loss: 0.7189 - accuracy: 0.7494 - val_loss: 0.8492 - val_accuracy: 0.7160\n",
      "Epoch 11/12\n",
      "40000/40000 [==============================] - 2s 60us/step - loss: 0.6733 - accuracy: 0.7627 - val_loss: 0.8376 - val_accuracy: 0.7248\n",
      "Epoch 12/12\n",
      "40000/40000 [==============================] - 2s 59us/step - loss: 0.6454 - accuracy: 0.7788 - val_loss: 0.8281 - val_accuracy: 0.7247\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = dataset.get_batch()\n",
    "trainer = Trainer(model, loss=\"categorical_crossentropy\", optimizer=RMSprop())\n",
    "trainer.train(\n",
    "    x_train, y_train, batch_size=128, epochs=12, validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.8370696809768676\n",
      "Test accuracy: 0.7193999886512756\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ネットワークをより深くする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(\n",
    "        32, kernel_size=3, padding=\"same\",\n",
    "        input_shape=input_shape, activation=\"relu\"\n",
    "        ))\n",
    "    # 32チャネル\n",
    "    model.add(Conv2D(32, kernel_size=3, activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    # 32チャネル\n",
    "    model.add(Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\"))\n",
    "    #64チャネル\n",
    "    model.add(Conv2D(64, kernel_size=3, activation=\"relu\"))\n",
    "    #64チャネル\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CIFAR10Dataset()\n",
    "model = network(dataset.image_shape, dataset.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "40000/40000 [==============================] - 3s 86us/step - loss: 1.8031 - accuracy: 0.3468 - val_loss: 1.7264 - val_accuracy: 0.3838\n",
      "Epoch 2/12\n",
      "40000/40000 [==============================] - 3s 77us/step - loss: 1.4014 - accuracy: 0.4970 - val_loss: 1.3247 - val_accuracy: 0.5276\n",
      "Epoch 3/12\n",
      "40000/40000 [==============================] - 3s 75us/step - loss: 1.2071 - accuracy: 0.5734 - val_loss: 1.0732 - val_accuracy: 0.6134\n",
      "Epoch 4/12\n",
      "40000/40000 [==============================] - 3s 76us/step - loss: 1.0669 - accuracy: 0.6244 - val_loss: 0.9595 - val_accuracy: 0.6600\n",
      "Epoch 5/12\n",
      "40000/40000 [==============================] - 3s 75us/step - loss: 0.9629 - accuracy: 0.6661 - val_loss: 0.9456 - val_accuracy: 0.6745\n",
      "Epoch 6/12\n",
      "40000/40000 [==============================] - 3s 76us/step - loss: 0.8861 - accuracy: 0.6899 - val_loss: 0.9652 - val_accuracy: 0.6735\n",
      "Epoch 7/12\n",
      "40000/40000 [==============================] - 3s 76us/step - loss: 0.8259 - accuracy: 0.7123 - val_loss: 0.8301 - val_accuracy: 0.7187\n",
      "Epoch 8/12\n",
      "40000/40000 [==============================] - 3s 77us/step - loss: 0.7688 - accuracy: 0.7329 - val_loss: 0.7882 - val_accuracy: 0.7374\n",
      "Epoch 9/12\n",
      "40000/40000 [==============================] - 3s 75us/step - loss: 0.7265 - accuracy: 0.7472 - val_loss: 0.7907 - val_accuracy: 0.7254\n",
      "Epoch 10/12\n",
      "40000/40000 [==============================] - 3s 75us/step - loss: 0.6960 - accuracy: 0.7602 - val_loss: 0.7022 - val_accuracy: 0.7586\n",
      "Epoch 11/12\n",
      "40000/40000 [==============================] - 3s 77us/step - loss: 0.6632 - accuracy: 0.7705 - val_loss: 0.6829 - val_accuracy: 0.7655\n",
      "Epoch 12/12\n",
      "40000/40000 [==============================] - 3s 75us/step - loss: 0.6397 - accuracy: 0.7767 - val_loss: 0.6865 - val_accuracy: 0.7737\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = dataset.get_batch()\n",
    "trainer = Trainer(model, loss=\"categorical_crossentropy\", optimizer=RMSprop())\n",
    "trainer.train(\n",
    "    x_train, y_train, batch_size=128, epochs=12, validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7125389976978302\n",
      "Test accuracy: 0.7699000239372253\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "\n",
    "    def __init__(self, model, loss, optimizer):\n",
    "        self._target = model\n",
    "        self._target.compile(\n",
    "            loss=loss, optimizer=optimizer, metrics=[\"accuracy\"]\n",
    "            )\n",
    "        self.verbose = 1\n",
    "        logdir = \"logdir_cifar10_deep_with_aug\"\n",
    "        self.log_dir = os.path.join(os.path.dirname(\"__file__\"), logdir)\n",
    "        self.model_file_name = \"model_file.hdf5\"\n",
    "\n",
    "    def train(self, x_train, y_train, batch_size, epochs, validation_split):\n",
    "        if os.path.exists(self.log_dir):\n",
    "            import shutil\n",
    "            shutil.rmtree(self.log_dir) \n",
    "        os.mkdir(self.log_dir)\n",
    "\n",
    "        datagen = ImageDataGenerator(\n",
    "            featurewise_center=False, \n",
    "            samplewise_center=False,  \n",
    "            featurewise_std_normalization=False,  \n",
    "            samplewise_std_normalization=False,  \n",
    "            zca_whitening=False,  \n",
    "            rotation_range=0,  \n",
    "            width_shift_range=0.1, # 横方向への移動割合\n",
    "            height_shift_range=0.1, # 縦方向への移動割合\n",
    "            horizontal_flip=True,  #水平方向への反転\n",
    "            vertical_flip=False)  \n",
    "\n",
    "        datagen.fit(x_train)# 学習データの平均、分散などの情報保存\n",
    "        # trainとvalidation分割\n",
    "        indices = np.arange(x_train.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        validation_size = int(x_train.shape[0] * validation_split)\n",
    "        x_train, x_valid = \\\n",
    "            x_train[indices[:-validation_size], :], \\\n",
    "            x_train[indices[-validation_size:], :]\n",
    "        y_train, y_valid = \\\n",
    "            y_train[indices[:-validation_size], :], \\\n",
    "            y_train[indices[-validation_size:], :]\n",
    "        \n",
    "        model_path = os.path.join(self.log_dir, self.model_file_name)\n",
    "        # 学習\n",
    "        self._target.fit_generator(\n",
    "            datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "            steps_per_epoch=x_train.shape[0] // batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=(x_valid, y_valid),\n",
    "            callbacks=[\n",
    "                TensorBoard(log_dir=self.log_dir),\n",
    "                ModelCheckpoint(model_path, save_best_only=True)\n",
    "            ],\n",
    "            verbose=self.verbose,\n",
    "            workers=4\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CIFAR10Dataset()\n",
    "model = network(dataset.image_shape, dataset.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "312/312 [==============================] - 10s 31ms/step - loss: 0.9082 - accuracy: 0.6899 - val_loss: 0.7116 - val_accuracy: 0.7558\n",
      "Epoch 2/15\n",
      "312/312 [==============================] - 10s 31ms/step - loss: 0.8935 - accuracy: 0.6963 - val_loss: 0.7157 - val_accuracy: 0.7575\n",
      "Epoch 3/15\n",
      "312/312 [==============================] - 10s 31ms/step - loss: 0.8820 - accuracy: 0.6982 - val_loss: 0.6680 - val_accuracy: 0.7732\n",
      "Epoch 4/15\n",
      "312/312 [==============================] - 10s 31ms/step - loss: 0.8693 - accuracy: 0.7038 - val_loss: 0.6946 - val_accuracy: 0.7658\n",
      "Epoch 5/15\n",
      "312/312 [==============================] - 10s 32ms/step - loss: 0.8661 - accuracy: 0.7076 - val_loss: 0.6876 - val_accuracy: 0.7753\n",
      "Epoch 6/15\n",
      "312/312 [==============================] - 10s 32ms/step - loss: 0.8565 - accuracy: 0.7078 - val_loss: 0.7782 - val_accuracy: 0.7424\n",
      "Epoch 7/15\n",
      "312/312 [==============================] - 10s 32ms/step - loss: 0.8527 - accuracy: 0.7090 - val_loss: 0.7812 - val_accuracy: 0.7379\n",
      "Epoch 8/15\n",
      "312/312 [==============================] - 10s 32ms/step - loss: 0.8531 - accuracy: 0.7112 - val_loss: 0.7989 - val_accuracy: 0.7522\n",
      "Epoch 9/15\n",
      "312/312 [==============================] - 10s 31ms/step - loss: 0.8453 - accuracy: 0.7142 - val_loss: 0.6640 - val_accuracy: 0.7738\n",
      "Epoch 10/15\n",
      "312/312 [==============================] - 10s 31ms/step - loss: 0.8401 - accuracy: 0.7181 - val_loss: 0.7378 - val_accuracy: 0.7622\n",
      "Epoch 11/15\n",
      "312/312 [==============================] - 10s 31ms/step - loss: 0.8282 - accuracy: 0.7213 - val_loss: 0.6534 - val_accuracy: 0.7762\n",
      "Epoch 12/15\n",
      "312/312 [==============================] - 10s 31ms/step - loss: 0.8254 - accuracy: 0.7231 - val_loss: 0.8295 - val_accuracy: 0.7335\n",
      "Epoch 13/15\n",
      "312/312 [==============================] - 10s 31ms/step - loss: 0.8338 - accuracy: 0.7195 - val_loss: 0.7270 - val_accuracy: 0.7578\n",
      "Epoch 14/15\n",
      "312/312 [==============================] - 10s 31ms/step - loss: 0.8272 - accuracy: 0.7223 - val_loss: 0.7430 - val_accuracy: 0.7609\n",
      "Epoch 15/15\n",
      "312/312 [==============================] - 10s 32ms/step - loss: 0.8381 - accuracy: 0.7186 - val_loss: 0.6843 - val_accuracy: 0.7691\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = dataset.get_batch()\n",
    "trainer = Trainer(model, loss=\"categorical_crossentropy\", optimizer=RMSprop())\n",
    "trainer.train(\n",
    "    x_train, y_train, batch_size=128, epochs=15, validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.7179710119247437\n",
      "Test accuracy: 0.7653999924659729\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"logdir_cifar10_deep_with_aug/model_file.hdf5\"\n",
    "images_folder = \"sample_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = load_model(model_path)\n",
    "image_shape = (32, 32, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images\n",
    "def crop_resize(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    length = min(image.size)\n",
    "    crop = image.crop((0, 0, length, length))\n",
    "    resized = crop.resize(image_shape[:2])  # use width x height\n",
    "    img = np.array(resized).astype(\"float32\")\n",
    "    img /= 255\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = Path(images_folder)\n",
    "image_paths = [str(f) for f in folder.glob(\"*.png\")]\n",
    "images = [crop_resize(p) for p in image_paths]\n",
    "images = np.asarray(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can detect cat & dog!\n"
     ]
    }
   ],
   "source": [
    "predicted = model.predict_classes(images)\n",
    "\n",
    "assert predicted[0] == 3, \"image should be cat.\"\n",
    "assert predicted[1] == 5, \"image should be dog.\"\n",
    "\n",
    "print(\"You can detect cat & dog!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
