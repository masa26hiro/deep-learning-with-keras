# 3章　畳み込みニューラルネットワーク

## 要約

### 前書き
- 全結合NNでは画像内の空間的な特徴が捉えられていない
  - 2次元の画像を1次元のベクトルに変換しているため、隣り合うピクセル同士の関連がうまく捉えられない
- CNNは空間的な特徴を捉えられる画像分類に適したNN
- テキスト、動画、音声など画像の領域を超えて利用されており大きな効果を上げている

### 3.1 CNNの仕組み
- CNNは以下の層が交互に重なり構成され、最終的には全結合層のネットワークに接続される
  - 畳み込み層
  - プーリング層(近年のモデルでは省略されることも多い)

#### 3.1.1 局所受容野
- CNNでは入力データ内の一定領域を次のレイヤーの1つのノードに接続する
- このノードを局所受容野という
- 一定領域の情報を1つに集約して接続することを畳み込みと呼ぶ
- 畳み込み演算の概要　別資料
- Kerasのパラメータとして以下を設定する
  - kernel_size : 畳み込む領域のサイズ
  - strides : フィルターを適用する位置の間隔
  - filter : 畳み込みで使用するフィルターの数
- 畳み込みによって作成されたデータを特徴マップと呼ぶ

#### 3.1.2 重みの共有
- 画像内の構造的特徴を捉えるアイデアとして重みのバイアスの共有がある
- 処理する場所によってフィルタの重みとバイアスは変わらない

#### 3.1.3 プーリング層
- 畳み込みによって作成された特徴マップのサイズを圧縮して扱いやすくする処理
- 別資料参照
- 具体的に紹介されているのは以下の2つ
1. Max-pooling
  - 指定した一定領域内の最大値をとる手法
  - 画像認識の分野では主にこれが使われている
2. Average pooling
  - 一定領域内の平均をとる手法

#### プーリング層の特徴
##### 学習するパラーメータがない
- プーリングは対象領域から最大値をとる（または平均をとる）だけの処理のため学習すべきパラメータを持たない

##### チャンネル数は変化しない
- プーリングの演算によって入力データと出力データのチャンネル数は変化しない

##### 微小な位置変化に対してロバスト
- 入力データの小さなずれに対してプーリングは同じような結果を返す
- そのため入力データの微小なずれに対してロバスト

### 3.2 CNNの実装例
- 最初期のCNNであるLeNetを実装する

#### 3.2.1 KerasによるLeNetの実装
- コード参照
- LeNetと現在のCNNの違い
  - LeNetでは活性化関数にシグモイド関数が使われている
  - Maxpoolingではなくサブサンプリングによって中間データのサイズ縮小を行っている

#### CNNの可視化
- CNNの畳み込み層は何を見ているか
- LeNetの第1層の重み(フィルタ)を学習前と学習後で比較する
- 学習前のフィルタは規則性がないが、学習後のフィルタには規則性のあるものになっている
  - グラデーション
  - 塊がある領域(blobと呼ばれる)
- フィルタはエッジやブロブを見ている
- フィルタはエッジやブロブなどプリミティブな情報を抽出し、その情報を後ろの層に渡していく

#### 3.2.2 ディープラーニングの力を理解する
- 学習データを減らした際に精度がどう変化するか観測する
- CNNは常に全結合NNよりも精度が高く、それは学習データが少ないほど顕著になる
