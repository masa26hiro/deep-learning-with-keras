# 1.1 パーセプトロン

## パーセプトロンとは
パーセプトロンは、重みω とバイアス bでなされる関数に複数の入力を与えることで、1 or 0 の出力を得られるアルゴリズム

参考になったURL -> [人工知能やディープラーニングの理解に欠かせないパーセプトロンとは何か？](https://qiita.com/yudsuzuk/items/a8e1eee92403f0921d92)

## Kerasコード例

Sequentialモデルを使ったKerasのコード例が以下

Dense関数で1層のネットワークを定義しており、8次元の入力に対して12次元の出力を定義しており、重みを -0.05 から 0.05 の範囲でランダムに初期化している

Dense関数の定義情報は以下にある
https://keras.io/ja/layers/core/

```
from keras.models import Sequential
from keras.layers import Dense
model = Sequential()
model.add(Dense(12,input_dim=8, kernel_initializer='random_uniform'))
```

# 1.2 多層パーセプトロン

## 多層パーセプトロンとは
複数の層をもつネットワークを指す

用語メモ
    決定境界：学習した機械学習モデルが作る分類の境界


## パーセプトロンの学習における課題と解決策
パーセプトロンは0 と 1の２つの値しか出力できないため、少しずつ改善に向かっているかどうか（漸次学習）ができず、学習に役立たない

0から1まで漸次的に変化する関数を使用することで、より細かな出力を得られる

## 活性化関数
後述のシグモイド関数やReLU関数などを指す
この関数を用いることによってネットワークの出力誤差を減らす事に役立たせることができる

その他の活性化関数については[ここ](https://qiita.com/namitop/items/d3d5091c7d0ab669195f)が参考になった
また、なぜ活性化関数に線形関数を使うべきではないかも記述されている

## シグモイド関数
入力によって0から1までの値を返す連続的な関数
変化が緩やかなため、小数点以下の値（0.5539など）も得ることができる

## ReLU関数
入力が負の時は常に0を返し、正の時は線形に増加する関数
