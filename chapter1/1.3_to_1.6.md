# ■ 損失関数
>## 前提知識
- **母集団**：対象のデータ全体のこと
- **母平均**：母集団の平均 ( <src img="https://latex.codecogs.com/gif.latex?\mu" /> )
- **対数の法則**：母集団から標本を抽出するとき、抽出する数が大きいほど、標本平均は母平均 <src img="https://latex.codecogs.com/gif.latex?\mu" /> に近づく
- **確率変数**：確率によって、様々な値をとるもの。<src img="https://latex.codecogs.com/gif.latex?P(X=2)=0.4" /> は 確率変数<src img="https://latex.codecogs.com/gif.latex?X" /> が実測値 <src img="https://latex.codecogs.com/gif.latex?x=2" /> をとる確率が0.2であることを表す。
- **期待値**：確率変数のすべての値に確率の重みをつけたか加重平均 ( <src img="https://latex.codecogs.com/gif.latex?E(X)" /> )。対数の法則により、期待値（母平均）は標本を大きくすると、標本平均に近づく。
- **分散**：確率変数値が全体として「平均」からどれだけ散らばっているか
- **訓練誤差**：学習に使った値と、モデルが予測した値の誤差
- **汎化誤差**：真のモデルと、学習モデルの誤差
- **バイアス**：学習モデルによる予測値と真のモデルの差の期待値
- **バリアンス（分散）**：学習モデルが予測した値の平均と個々の予測値がどれだけ離れているか。モデルが複雑になるほど高くなる = overfitしている。バイアスとバリアンスはトレードオフの関係。

>## MES(二乗平均誤差)
- 真の値：<src img="https://latex.codecogs.com/gif.latex?y=f(x)&plus;\epsilon" />、<src img="https://latex.codecogs.com/gif.latex?\epsilon..." />平均 : 0, 分散 : <src img="https://latex.codecogs.com/gif.latex?\sigma^2" />
- 予測値：<src img="https://latex.codecogs.com/gif.latex?\hat{y}=\hat{f}(x)" />

バイアスとバリアンスをノイズの分散 <src img="https://latex.codecogs.com/gif.latex?\sigma" /> 和で表せる  
<src img="https://latex.codecogs.com/gif.latex?MSE=V[\hat{f}(x)]&plus;B[\hat{f}(x)]^2&plus;\sigma^2" />

>## 交差エントロピー
### $E = -\sum_kq(k)log(p(k))$
確率分布$q(k)$, $p(k)$の近似性を表現  
分類問題の時は、MSEよりも収束が早い

>## バイナリ交差エントロピー
- 二値分類問題のとき
- $P(x_1) = p$ , $P(x_2) = 1 - p$
- $Q(x_1) = q$ , $Q(x_2) = 1 - q$

>## カテゴリカル交差エントロピー
- 多分類問題のとき
- $P(x_i) = p_i (i = 1, 2, ...)$
- $Q(x_i) = q_i (i = 1, 2, ...)$


---
# ■ 分類問題の評価関数
- 混同行列  
  - T(True)は予測正解、F(False)は予測不正解。
  - Pは予測が正(Positive)、Nは予測が負(Negative)

||実際は正|実際は負|
|:--:|:--:|:--:|
|予測が正|TP|FP|
|予測が負|FN|TN|

>精度(Accuracy)
## $\frac{TP+TN}{TP+FP+FN+TN}$

正や負と予測したデータのうち，実際にそうであるものの割合

>適合率(Precision)
## $\frac{TP}{TP+FP}$

正と予測したデータのうち，実際に正であるものの割合

>再現率(Recall)
## $\frac{TP}{TP+FN}$

実際に正であるもののうち，正であると予測されたものの割合

>特異率(Specificity)
## $\frac{TN}{FP+TN}$

実際に負であるもののうち，負であると予測されたものの割合

>F値
## $\frac{2Recall × Precision}{Recall+Precision}$

精度と再現率の調和平均

---
# ■ ドロップアウト
- 過学習の緩和
- 複数モデルの組み合わせによる精度の向上(Baggingの要領)
- 確率 $p$で$1$、確率 $p-1$で$0$を出力にかける

---
# ■　最適化
> ## 反復法
- ### $MSE(X, f_w(x)) = MSE(w)$
- ### $w^{k+1} = w^k + \alpha^kd^k$

$X$：全データ  
$f_w(x)$：重み$w$の関数  (ex. : $f_w(x)=w^Tx$)   
$w^{k+1}$ : 重み  
$\alpha^k$：ステップ幅  
$d^k$：探索方向  

>最急降下法  

$d^k$ : 損失関数の勾配ベクトル（重みで微分)

学習データのすべての誤差の合計を取ってからパラメーターを更新する。

>確率的勾配降下法

最急降下法ではすべてのデータを使って勾配を計算するが、
確率的勾配ではランダムに1つのデータを使う。またはミニバッチ
数分のデータを使う。

---
# ■　正則化
・モデルの自由度を減らして、モデルが複雑になりすぎるのを防ぐ  
・それぞれの層に対して定義

> L1正則化
- ペナルティ項：$\lambda\sum_{i=1}^n|w_i|$
    - $w_i$が小さくなる(0になる)ように学習 → モデルのスパース化
    - 重要性の低い特徴量の重みを取り除く
    - 複数の相関が強い説明変数が存在する場合にはそのグループの中で一つの変数のみを選択してしまうという欠点がある
> L2正則化
- ペナルティ項：$\frac{\lambda}{2}\sum_{i=1}^n|w_i|^2$
    - 重みが大きすぎるものに大きなペナルティを与える → モデルの平滑化
    - 重みは完全には0にならない、パラメータの数が多いと、結局過学習

> Elastic Net
- ペナルティ項：$r\alpha\sum_{i=1}^n{|w_i|}+\frac{1-r}{2}\alpha\sum_{i=0}^{n}|w_i|^2$
    - L1正則化とL2正則化を$r$の割合で混ぜる
    - L1正則化の不安定さ解消
    - L2正則化のパラメータの数に制約がある問題点を克服
---
# ■誤差逆伝播法
損失関数の微分を効率よく計算する方法

> ## 数値微分
微分の定義に従って、近似を求める
- 誤差に弱い
- 計算量が多い

> ## 自動微分
プログラムによって定義された数学的な関数が与えられたときに、その導関数をアルゴリズムによって機械的に求める手法

## $f(x_1, x_2) = x_1x_2 + \sin x_2$  

- 分解
    - $w_1←x_1$
    - $w_2←x_2$
    - $w_3←w_1w_2$
    - $w_4←\sin w_1$
    - $w_5←w_3+w_4$

- 求めたいもの
    - ## $\frac{\partial f(x_1, x_2)}{\partial x_1}$
    - ## $\frac{\partial f(x_1, x_2)}{\partial x_2}$

>フォワードモード自動微分

・入力変数に対して、すべての中間変数に対する偏導関数値を計算していく手法
・出力パラメータが多く、入力パラメータが少ないときに有効

### 1. $\frac{\partial w_1}{\partial x_1}=1$
### 2. $\frac{\partial w_2}{\partial x_1}=0$
### 3. $\frac{\partial w_3}{\partial x_1}=\frac{\partial (w_1w_2)}{\partial x_1}=\frac{\partial w_1}{\partial x_1}w_2+w_1\frac{\partial w_2}{\partial x_1}$
### 4. $\frac{\partial w_4}{\partial x_1}=\frac{\partial \sin w_1}{\partial x_1}=\frac{\partial w_1}{\partial x_1}\cos w_1$
### 5. $\frac{\partial w_5}{\partial x_1}=\frac{\partial (w_3+w_4)}{\partial x_1}=\frac{\partial w_3}{\partial x_1}+\frac{\partial w_4}{\partial x_1}$

>リバースモード自動微分

・一つの出力変数について、全ての中間変数に対する偏導関数値を計算していく手法  
・誤差逆伝播法はリバースモード自動微分の一種  
・入力パラメータが多く、出力パラメータが少ないときに有効

### 1. $\frac{\partial w_5}{\partial w_5}=1$
### 2. $\frac{\partial w_5}{\partial w_4}=\frac{\partial w_5}{\partial w_5}\frac{\partial w_5}{\partial w_4}=\frac{\partial w_5}{\partial w_5}\frac{\partial (w_3+w_4)}{\partial w_4}=\frac{\partial w_5}{\partial w_5}$
### 3. $\frac{\partial w_5}{\partial w_3}=\frac{\partial w_5}{\partial w_5}\frac{\partial (w_3+w_4)}{\partial w_3}=\frac{\partial w_5}{\partial w_5}$
### 4. $\frac{\partial w_5}{\partial w_2}=\frac{\partial w_5}{\partial w_3}\frac{\partial w_3}{\partial w_2}=\frac{\partial w_5}{\partial w_3}\frac{\partial (w_1w_2)}{\partial w_2}=\frac{\partial w_5}{\partial w_3}w_1$
### 5. $\frac{\partial w_5}{\partial w_1}=\frac{\partial w_5}{\partial w_3}\frac{\partial w_3}{\partial w_1}+\frac{\partial w_5}{\partial w_4}\frac{\partial w_4}{\partial w_1}=\frac{\partial w_5}{\partial w_3}\frac{\partial (w_1w_2)}{\partial w_1}+\frac{\partial w_5}{\partial w_4}\frac{\partial \sin w_1}{\partial w_1}=\frac{\partial w_5}{\partial w_3}w_2+\frac{\partial w_5}{\partial w_4}\cos w_1$
