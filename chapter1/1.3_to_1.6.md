# ■ 損失関数
>## 前提知識
- **母集団**：対象のデータ全体のこと
- **母平均**：母集団の平均 ( <img src="https://latex.codecogs.com/svg.latex?\mu" /> )
- **対数の法則**：母集団から標本を抽出するとき、抽出する数が大きいほど、標本平均は母平均 <img src="https://latex.codecogs.com/svg.latex?\mu" /> に近づく
- **確率変数**：確率によって、様々な値をとるもの。<img src="https://latex.codecogs.com/svg.latex?P(X=2)=0.4" /> は 確率変数<img src="https://latex.codecogs.com/svg.latex?X" /> が実測値 <img src="https://latex.codecogs.com/svg.latex?x=2" /> をとる確率が0.2であることを表す。
- **期待値**：確率変数のすべての値に確率の重みをつけたか加重平均 ( <img src="https://latex.codecogs.com/svg.latex?E(X)" /> )。対数の法則により、期待値（母平均）は標本を大きくすると、標本平均に近づく。
- **分散**：確率変数値が全体として「平均」からどれだけ散らばっているか
- **訓練誤差**：学習に使った値と、モデルが予測した値の誤差
- **汎化誤差**：真のモデルと、学習モデルの誤差
- **バイアス**：学習モデルによる予測値と真のモデルの差の期待値
- **バリアンス（分散）**：学習モデルが予測した値の平均と個々の予測値がどれだけ離れているか。モデルが複雑になるほど高くなる = overfitしている。バイアスとバリアンスはトレードオフの関係。

>## MES(二乗平均誤差)
- 真の値：<img src="https://latex.codecogs.com/svg.latex?y=f(x)&plus;\epsilon" />、<img src="https://latex.codecogs.com/svg.latex?\epsilon..." />平均 : 0, 分散 : <img src="https://latex.codecogs.com/svg.latex?\sigma^2" />
- 予測値：<img src="https://latex.codecogs.com/svg.latex?\hat{y}=\hat{f}(x)" />

バイアスとバリアンスをノイズの分散 <img src="https://latex.codecogs.com/svg.latex?\sigma" /> 和で表せる  
<img src="https://latex.codecogs.com/svg.latex?MSE=V[\hat{f}(x)]&plus;B[\hat{f}(x)]^2&plus;\sigma^2" />

>## 交差エントロピー
### <img src="https://latex.codecogs.com/svg.latex?E&space;=&space;-\sum_kq(k)log(p(k))" />
確率分布<img src="https://latex.codecogs.com/svg.latex?q(k)" />, <img src="https://latex.codecogs.com/svg.latex?p(k)" />の近似性を表現  
分類問題の時は、MSEよりも収束が早い

>## バイナリ交差エントロピー
- 二値分類問題のとき
- <img src="https://latex.codecogs.com/svg.latex?P(x_1)&space;=&space;p&space;,&space;P(x_2)&space;=&space;1&space;-&space;p" />
- <img src="https://latex.codecogs.com/svg.latex?Q(x_1)&space;=&space;q&space;,&space;Q(x_2)&space;=&space;1&space;-&space;q" />

>## カテゴリカル交差エントロピー
- 多分類問題のとき
- <img src="https://latex.codecogs.com/svg.latex?P(x_i)&space;=&space;p_i&space;(i&space;=&space;1,&space;2,&space;...)" />
- <img src="https://latex.codecogs.com/svg.latex?Q(x_i)&space;=&space;q_i&space;(i&space;=&space;1,&space;2,&space;...)" />


---
# ■ 評価関数
> ## 二値分類
- 混同行列  
  - T(True)は予測正解、F(False)は予測不正解。
  - Pは予測が正(Positive)、Nは予測が負(Negative)

||実際は正|実際は負|
|:--:|:--:|:--:|
|予測が正|TP|FP|
|予測が負|FN|TN|

>精度(Accuracy)
### <img src="https://latex.codecogs.com/svg.latex?\frac{TP&plus;TN}{TP&plus;FP&plus;FN&plus;TN}" />

正や負と予測したデータのうち，実際にそうであるものの割合

>適合率(Precision)
### <img src="https://latex.codecogs.com/svg.latex?\frac{TP}{TP&plus;FP}" />

正と予測したデータのうち，実際に正であるものの割合

>再現率(Recall)
### <img src="https://latex.codecogs.com/svg.latex?\frac{TP}{TP&plus;FN}" />

実際に正であるもののうち，正であると予測されたものの割合

>特異率(Specificity)
### <img src="https://latex.codecogs.com/svg.latex?\frac{TN}{FP&plus;TN}" />

実際に負であるもののうち，負であると予測されたものの割合

>F値
### <img src="https://latex.codecogs.com/svg.latex?\frac{2Recall&space;×&space;Precision}{Recall&plus;Precision}" />

精度と再現率の調和平均

>その他
- ROC curve
- AUC
- Logarithmic Loss

> ## 多クラス分類
- 混同行列  
    - TPi (真陽性) : クラス Ci の正しく認識された観測数 
    - TNi (真陰性) : クラス Ci に属していない正しく認識された観測数
    - FPi (偽陽性) : クラス Ci に誤って割り当てられた観測数
    - FNi (偽陰性) : クラス Ci に属していると認識されなかった観測数

> 指標
- 正解率
- マクロ平均再現率(クラスごとの再現率の平均)
- マクロ平均適合率(クラスごとの適合率の平均)
- マクロ平均F値(マクロ平均適合率とマクロ平均再現率の調和平均)
- マイクロ平均再現率(多クラス混同行列では正解率と等しい)
- マイクロ平均適合率(多クラス混同行列では正解率と等しい)
- マイクロ平均F値正解率(多クラス混同行列では正解率と等しい)

> ## 数値
- Mean Squared Error(平均二乗誤差)
- Root Mean Squared Error(平均二乗平方根誤差)
- Mean Absolute Error(平均絶対誤差)
- Coefficient of Determination(決定係数)
- AIC(赤池情報基準)
- BIC(ベイズ情報基準)

> ## 時系列
- MSE RMSE MAE
- Dynamic Time Warping(DTW) Distance(動的時間伸縮法)
- Perprexity(パープレキシティ)
- Regret(リグレット)

>参考  

- [機械学習で使う指標総まとめ(教師あり学習編)](https://www.procrasist.com/entry/ml-metrics)  
- [多クラス分類アルゴリズムの品質メトリック](https://jp.xlsoft.com/documents/intel/daal/2016/daal2016_ref/GUID-5220F4E2-3169-40D7-A3B2-CCF10C27B3FE.htm)
- [多クラス混同行列とその評価指標〜マクロ平均、マイクロ平均〜](https://analysis-navi.com/?p=553#i-4)


---
# ■ ドロップアウト
- 過学習の緩和
- 複数モデルの組み合わせによる精度の向上(Baggingの要領)
- 確率 pで1、確率 p-1で0を出力にかける

---
# ■　最適化
> ## 反復法
<img src="https://latex.codecogs.com/svg.latex?MSE(X,&space;f_w(x))&space;=&space;MSE(w)" /><br>
<img src="https://latex.codecogs.com/svg.latex?w^{k&plus;1}&space;=&space;w^k&space;&plus;&space;\alpha^kd^k" />

<img src="https://latex.codecogs.com/svg.latex?X" />：全データ  
<img src="https://latex.codecogs.com/svg.latex?f_w(x)" />：重みwの関数  (ex. : <img src="https://latex.codecogs.com/svg.latex?f_w(x)=w^Tx" />)   
<img src="https://latex.codecogs.com/svg.latex?w^{k+1}" /> : 重み  
<img src="https://latex.codecogs.com/svg.latex?\alpha^k" />：ステップ幅  
<img src="https://latex.codecogs.com/svg.latex?d^k" />：探索方向  

>最急降下法  

<img src="https://latex.codecogs.com/svg.latex?d^k" /> : 損失関数の勾配ベクトル（重みで微分)

学習データのすべての誤差の合計を取ってからパラメーターを更新する。

>確率的勾配降下法

最急降下法ではすべてのデータを使って勾配を計算するが、
確率的勾配ではランダムに1つのデータを使う。またはミニバッチ
数分のデータを使う。

---
# ■　正則化
・モデルの自由度を減らして、モデルが複雑になりすぎるのを防ぐ  
・それぞれの層に対して定義

> L1正則化

・ペナルティ項：  
<img src="https://latex.codecogs.com/svg.latex?\lambda\sum_{i=1}^n|w_i|" /><br>
・$w_i$が小さくなる(0になる)ように学習 → モデルのスパース化  
・重要性の低い特徴量の重みを取り除く  
・複数の相関が強い説明変数が存在する場合にはそのグループの中で一つの変数のみを選択してしまうという欠点がある
> L2正則化

・ ペナルティ項：

<img src="https://latex.codecogs.com/svg.latex?\frac{\lambda}{2}\sum_{i=1}^n|w_i|^2" /><br>
・ 重みが大きすぎるものに大きなペナルティを与える → モデルの平滑化  
・ 重みは完全には0にならない、パラメータの数が多いと、結局過学習

> Elastic Net

・ ペナルティ項：

<img src="https://latex.codecogs.com/svg.latex?r\alpha\sum_{i=1}^n{|w_i|}&plus;\frac{1-r}{2}\alpha\sum_{i=0}^{n}|w_i|^2" /><br>
・ L1正則化とL2正則化をrの割合で混ぜる  
・ L1正則化の不安定さ解消  
・ L2正則化のパラメータの数に制約がある問題点を克服

---

# ■誤差逆伝播法
損失関数の微分を効率よく計算する方法

> ## 数値微分
微分の定義に従って、近似を求める
- 誤差に弱い
- 計算量が多い

> ## 自動微分
プログラムによって定義された数学的な関数が与えられたときに、その導関数をアルゴリズムによって機械的に求める手法

## <img src="https://latex.codecogs.com/svg.latex?f(x_1,&space;x_2)&space;=&space;x_1x_2&space;&plus;&space;\sin&space;x_2" />

- 分解
    - <img src="https://latex.codecogs.com/svg.latex?w_1\leftarrow&space;x_1" />
    - <img src="https://latex.codecogs.com/svg.latex?w_2\leftarrow&space;x_2" />
    - <img src="https://latex.codecogs.com/svg.latex?w_3\leftarrow&space;w_1w_2" />
    - <img src="https://latex.codecogs.com/svg.latex?w_4\leftarrow&space;\sin&space;w_1" />
    - <img src="https://latex.codecogs.com/svg.latex?w_5\leftarrow&space;w_3&plus;w_4" />

- 求めたいもの
    - ## <img src="https://latex.codecogs.com/svg.latex?\frac{\partial&space;f(x_1,&space;x_2)}{\partial&space;x_1}" />
    - ## <img src="https://latex.codecogs.com/svg.latex?\frac{\partial&space;f(x_1,&space;x_2)}{\partial&space;x_2}" />

>フォワードモード自動微分

・入力変数に対して、すべての中間変数に対する偏導関数値を計算していく手法
・出力パラメータが多く、入力パラメータが少ないときに有効

### 1. <img src="https://latex.codecogs.com/svg.latex?\frac{\partial&space;w_1}{\partial&space;x_1}=1" />
### 2. <img src="https://latex.codecogs.com/svg.latex?\frac{\partial&space;w_2}{\partial&space;x_1}=0" />
### 3. <img src="https://latex.codecogs.com/svg.latex?\frac{\partial&space;w_3}{\partial&space;x_1}=\frac{\partial&space;(w_1w_2)}{\partial&space;x_1}=\frac{\partial&space;w_1}{\partial&space;x_1}w_2&plus;w_1\frac{\partial&space;w_2}{\partial&space;x_1}" />
### 4. <img src="https://latex.codecogs.com/svg.latex?\frac{\partial&space;w_4}{\partial&space;x_1}=\frac{\partial&space;\sin&space;w_1}{\partial&space;x_1}=\frac{\partial&space;w_1}{\partial&space;x_1}\cos&space;w_1" />
### 5. <img src="https://latex.codecogs.com/svg.latex?\frac{\partial&space;w_5}{\partial&space;x_1}=\frac{\partial&space;(w_3&plus;w_4)}{\partial&space;x_1}=\frac{\partial&space;w_3}{\partial&space;x_1}&plus;\frac{\partial&space;w_4}{\partial&space;x_1}" />

>リバースモード自動微分

・一つの出力変数について、全ての中間変数に対する偏導関数値を計算していく手法  
・誤差逆伝播法はリバースモード自動微分の一種  
・入力パラメータが多く、出力パラメータが少ないときに有効

### 1. <img src="https://latex.codecogs.com/svg.latex?\frac{\partial&space;w_5}{\partial&space;w_5}=1" />
### 2. <img src="https://latex.codecogs.com/svg.latex?\frac{\partial&space;w_5}{\partial&space;w_4}=\frac{\partial&space;w_5}{\partial&space;w_5}\frac{\partial&space;w_5}{\partial&space;w_4}=\frac{\partial&space;w_5}{\partial&space;w_5}\frac{\partial&space;(w_3&plus;w_4)}{\partial&space;w_4}=\frac{\partial&space;w_5}{\partial&space;w_5}" />
### 3. <img src="https://latex.codecogs.com/svg.latex?\frac{\partial&space;w_5}{\partial&space;w_3}=\frac{\partial&space;w_5}{\partial&space;w_5}\frac{\partial&space;(w_3&plus;w_4)}{\partial&space;w_3}=\frac{\partial&space;w_5}{\partial&space;w_5}" />
### 4. <img src="https://latex.codecogs.com/svg.latex?\frac{\partial&space;w_5}{\partial&space;w_2}=\frac{\partial&space;w_5}{\partial&space;w_3}\frac{\partial&space;w_3}{\partial&space;w_2}=\frac{\partial&space;w_5}{\partial&space;w_3}\frac{\partial&space;(w_1w_2)}{\partial&space;w_2}=\frac{\partial&space;w_5}{\partial&space;w_3}w_1" />

### 5. <img src="https://latex.codecogs.com/svg.latex?\frac{\partial&space;w_5}{\partial&space;w_1}=\frac{\partial&space;w_5}{\partial&space;w_3}\frac{\partial&space;w_3}{\partial&space;w_1}&plus;\frac{\partial&space;w_5}{\partial&space;w_4}\frac{\partial&space;w_4}{\partial&space;w_1}=\frac{\partial&space;w_5}{\partial&space;w_3}\frac{\partial&space;(w_1w_2)}{\partial&space;w_1}&plus;\frac{\partial&space;w_5}{\partial&space;w_4}\frac{\partial&space;\sin&space;w_1}{\partial&space;w_1}=" /><img src="https://latex.codecogs.com/svg.latex?\frac{\partial&space;w_5}{\partial&space;w_3}w_2&plus;\frac{\partial&space;w_5}{\partial&space;w_4}\cos&space;w_1" />
