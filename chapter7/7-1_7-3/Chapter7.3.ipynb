{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlretrieve\n",
    "from collections import Counter\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.layers import Input, LSTM, Bidirectional, RepeatVector\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ読み込みクラス\n",
    "class ReutersCorpus():\n",
    "    def __init__(self, padding=\"PAD\", unknown=\"UNK\"):\n",
    "        self.documents = []\n",
    "        self.stopwords = []\n",
    "        self.vocab = []\n",
    "        self._ignores = re.compile(\"[.,-/\\\"'>()&;:]\")\n",
    "        # 文字列の長さを揃えるために使用\n",
    "        self.PAD = padding\n",
    "        # 辞書にない単語\n",
    "        self.UNK = unknown\n",
    "        try:\n",
    "            self.documents = reuters.fileids()\n",
    "        except LookupError:\n",
    "            print(\"Reuters corpus does not downloaded. So download it.\")\n",
    "            nltk.download(\"reuters\")\n",
    "            self.documents = reuters.fileids()\n",
    "        \n",
    "        try:\n",
    "            # 出現頻度が高く、かつ分の特徴にならない英語のリスト\n",
    "            self.stopwords = stopwords.words(\"english\")\n",
    "        except LookupError:\n",
    "            print(\"English stopword does not downloaded. So download it.\")\n",
    "            nltk.download(\"stopwords\")\n",
    "            self.stopwords = stopwords.words(\"english\")\n",
    "    \n",
    "    # 辞書の作成\n",
    "    def build(self, vocab_size=5000):\n",
    "        words = reuters.words()\n",
    "        words = [self.trim(w) for w in words]\n",
    "        words = [w for w in words if w]\n",
    "        freq = Counter(words)\n",
    "        freq = freq.most_common(vocab_size)\n",
    "        self.vocab = [w_c[0] for w_c in freq]\n",
    "        self.vocab = [self.PAD, self.UNK] + self.vocab\n",
    "    \n",
    "    # 不要な語の除去\n",
    "    def trim(self, word):\n",
    "        w = word.lower().strip()\n",
    "        if w in self.stopwords or self._ignores.match(w):\n",
    "            return \"\"\n",
    "        if w.replace(\".\", \"\").isdigit():\n",
    "            return \"9\"\n",
    "        return w\n",
    "    \n",
    "    def batch_iter(self, embedding, kind=\"train\", batch_size=64, seq_size=50):\n",
    "        if len(self.vocab) == 0:\n",
    "            raise Exception(\"Vocabulary hasn't made yet. Please execute 'build' method.\")\n",
    "        steps = self.get_step_count(kind, batch_size)\n",
    "        docs = self.get_documents(kind)\n",
    "        docs_i = self.docs_to_matrix(docs, seq_size)\n",
    "        docs = None # free memory\n",
    "\n",
    "        while True:\n",
    "            indices = np.random.permutation(np.arange(len(docs_i)))\n",
    "            for s in range(steps):\n",
    "                index = s * batch_size\n",
    "                x = docs_i[indices[index:(index + batch_size)]]\n",
    "                # 単語の数値を、事前学習済みの単語ベクトルに変換する（markdownへ...）\n",
    "                x_vec = embedding[x]\n",
    "                # input = output\n",
    "                yield x_vec, x_vec\n",
    "    \n",
    "    # 単語配列の作成\n",
    "    def docs_to_matrix(self, docs, seq_size):\n",
    "        docs_i = []\n",
    "        for d in docs:\n",
    "            # 文を単語に分割する\n",
    "            words = reuters.words(d)\n",
    "            words = self.sentence_to_ids(words, seq_size)\n",
    "            docs_i.append(words)\n",
    "        docs_i = np.array(docs_i)\n",
    "        return docs_i\n",
    "    \n",
    "    # 文章を文に\n",
    "    def sentence_to_ids(self, sentence, seq_size):\n",
    "        v = self.vocab\n",
    "        UNK = v.index(self.UNK)\n",
    "        PAD = v.index(self.PAD)\n",
    "        # 文章を一定の長さに切り取る\n",
    "        words = [self.trim(w) for w in sentence][:seq_size]\n",
    "        # 辞書を使って単語を数値に変換する（なければUNKに）\n",
    "        words = [v.index(w) if w in v else UNK for w in words if w]\n",
    "        if len(words) < seq_size:\n",
    "            # 文字の長さを揃える\n",
    "            words += [PAD] * (seq_size - len(words))\n",
    "        return words\n",
    "\n",
    "    def get_step_count(self, kind=\"train\", batch_size=64):\n",
    "        size = len(self.get_documents(kind))\n",
    "        return size // batch_size\n",
    "    \n",
    "    def get_documents(self, kind=\"train\"):\n",
    "        docs = list(filter(lambda doc: doc.startswith(kind), self.documents))\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事前学習済みの単語ベクトルを読み込むクラス\n",
    "class EmbeddingLoader():\n",
    "    def __init__(self, embed_dir=\"\", size=100):\n",
    "        self.embed_dir = embed_dir\n",
    "        self.size = size\n",
    "        if not self.embed_dir:\n",
    "            self.embed_dir = os.path.join(os.path.dirname(\"__file__\"), \"embed\")\n",
    "    \n",
    "    def load(self, seq_size, corpus, download=True):\n",
    "        url = \"https://nlp.stanford.edu/data/wordvecs/glove.6B.zip\"\n",
    "        embed_name = \"glove.6B.{}d.txt\".format(self.size)\n",
    "        embed_path = os.path.join(self.embed_dir, embed_name)\n",
    "        if not os.path.isfile(embed_path):\n",
    "            if not download:\n",
    "                raise Exception(\"Can't load embedding from {}.\".format(embed_path))\n",
    "            else:\n",
    "                print(\"Download the GloVe embedding.\")\n",
    "                file_name = os.path.basename(url)\n",
    "                if not os.path.isdir(self.embed_dir):\n",
    "                    os.mkdir(self.embed_dir)\n",
    "                zip_path = os.path.join(self.embed_dir, file_name)\n",
    "                urlretrieve(url, zip_path)\n",
    "                with ZipFile(zip_path) as z:\n",
    "                    z.extractall(self.embed_dir)\n",
    "                    # エラー起きるのでコメントアウト\n",
    "                    # os.remove(zip_path)\n",
    "        \n",
    "        vocab = corpus.vocab\n",
    "        if len(vocab) == 0:\n",
    "            raise Exception(\"You have to make vocab by 'build' method.\")\n",
    "        embed_matrix = np.zeros((len(vocab), self.size))\n",
    "        UNK = vocab.index(corpus.UNK)\n",
    "        with open(embed_path, mode=\"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                values = line.strip().split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "                if word in vocab:\n",
    "                    index = vocab.index(word)\n",
    "                    embed_matrix[index] = vector\n",
    "        embed_matrix[UNK] = np.random.uniform(-1, 1, self.size)\n",
    "        return embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自己符号化器\n",
    "class AutoEncoder():\n",
    "    \n",
    "    def __init__(self, seq_size=50, embed_size=100, latent_size=256):\n",
    "        self.seq_size = seq_size\n",
    "        self.embed_size = embed_size\n",
    "        self.latent_size = latent_size\n",
    "        self.model = None\n",
    "    \n",
    "    def build(self):\n",
    "        # 双方向（Bidirectional）LSTMでのエンコーディング\n",
    "        inputs = Input(shape=(self.seq_size, self.embed_size), name=\"input\")\n",
    "        encoded = Bidirectional(LSTM(self.latent_size), merge_mode=\"concat\", name=\"encoder\")(inputs)\n",
    "        # エンコーダーからの出力を単語数分複製する\n",
    "        encoded = RepeatVector(self.seq_size, name=\"replicate\")(encoded)\n",
    "        # 双方向（Bidirectional）LSTMでの復号\n",
    "        decoded = Bidirectional(LSTM(self.embed_size, return_sequences=True), merge_mode=\"sum\", name=\"devoder\")(encoded)\n",
    "        self.model = Model(inputs, decoded)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        model = load_model(path)\n",
    "        _, seq_size, embed_size = model.input.shape  # top is batch size\n",
    "        latent_size = model.get_layer(\"encoder\").input_shape[1]\n",
    "        ae = AutoEncoder(seq_size, embed_size, latent_size)\n",
    "        ae.model = model\n",
    "        return ae\n",
    "    \n",
    "    # エンコーダーだけの取得\n",
    "    def get_encoder(self):\n",
    "        if self.model:\n",
    "            m = self.model\n",
    "            encoder = Model(m.input, m.get_layer(\"encoder\").output)\n",
    "            return encoder\n",
    "        else:\n",
    "            raise Exception(\"Model is not built/loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(log_dir, model_name=\"autoencoder.h5\"):\n",
    "    print(\"1. Prepare the corpus.\")\n",
    "    corpus = ReutersCorpus()\n",
    "    corpus.build(vocab_size=5000)\n",
    "    \n",
    "    print(\"2. Make autoencoder model.\")\n",
    "    ae = AutoEncoder(seq_size=50, embed_size=100, latent_size=512)\n",
    "    ae.build()\n",
    "    \n",
    "    print(\"3. Load GloVe embeddings.\")\n",
    "    embed_loader = EmbeddingLoader(size=ae.embed_size)\n",
    "    embedding = embed_loader.load(ae.seq_size, corpus)\n",
    "    \n",
    "    print(\"4. Train the model (trained model is saved to {}).\".format(log_dir))\n",
    "    batch_size = 64\n",
    "    ae.model.compile(optimizer=\"sgd\", loss=\"mse\")\n",
    "    model_file = os.path.join(log_dir, model_name)\n",
    "    train_iter = corpus.batch_iter(embedding, \"train\", batch_size, ae.seq_size)\n",
    "    test_iter = corpus.batch_iter(embedding, \"test\", batch_size, ae.seq_size)\n",
    "    train_steps = corpus.get_step_count(\"train\", batch_size)\n",
    "    test_steps = corpus.get_step_count(\"test\", batch_size)\n",
    "    \n",
    "    ae.model.fit_generator(\n",
    "        train_iter, train_steps,\n",
    "        epochs=10,\n",
    "        validation_data=test_iter,\n",
    "        validation_steps=test_steps,\n",
    "        callbacks=[\n",
    "            TensorBoard(log_dir=log_dir),\n",
    "            ModelCheckpoint(filepath=model_file, save_best_only=True)\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Prepare the corpus.\n",
      "2. Make autoencoder model.\n",
      "3. Load GloVe embeddings.\n",
      "4. Train the model (trained model is saved to ./logs).\n",
      "WARNING:tensorflow:From <ipython-input-5-3372d3c912cc>:30: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 121 steps, validate for 47 steps\n",
      "Epoch 1/10\n",
      "121/121 [==============================] - 156s 1s/step - loss: 0.2119 - val_loss: 0.2039\n",
      "Epoch 2/10\n",
      "121/121 [==============================] - 154s 1s/step - loss: 0.1941 - val_loss: 0.1920\n",
      "Epoch 3/10\n",
      "121/121 [==============================] - 146s 1s/step - loss: 0.1865 - val_loss: 0.1878\n",
      "Epoch 4/10\n",
      "121/121 [==============================] - 151s 1s/step - loss: 0.1826 - val_loss: 0.1841\n",
      "Epoch 5/10\n",
      "121/121 [==============================] - 152s 1s/step - loss: 0.1803 - val_loss: 0.1821\n",
      "Epoch 6/10\n",
      "121/121 [==============================] - 144s 1s/step - loss: 0.1789 - val_loss: 0.1812\n",
      "Epoch 7/10\n",
      "121/121 [==============================] - 143s 1s/step - loss: 0.1780 - val_loss: 0.1801\n",
      "Epoch 8/10\n",
      "121/121 [==============================] - 148s 1s/step - loss: 0.1773 - val_loss: 0.1796\n",
      "Epoch 9/10\n",
      "121/121 [==============================] - 157s 1s/step - loss: 0.1767 - val_loss: 0.1791\n",
      "Epoch 10/10\n",
      "121/121 [==============================] - 161s 1s/step - loss: 0.1763 - val_loss: 0.1783\n"
     ]
    }
   ],
   "source": [
    "main(\"./logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# エンコーダを使用した文書分類（クラスタリング）\n",
    "def predict(log_dir, model_name=\"autoencoder.h5\"):\n",
    "    print(\"1. Load the trained model.\")\n",
    "    model_file = os.path.join(log_dir, model_name)\n",
    "    ae = AutoEncoder.load(model_file)\n",
    "    \n",
    "    print(\"2. Prepare the corpus.\")\n",
    "    corpus = ReutersCorpus()\n",
    "    test_docs = corpus.get_documents(\"test\")\n",
    "    labels = [reuters.categories(f)[0] for f in test_docs]\n",
    "    categories = Counter(labels).most_common()\n",
    "    # Use categories that has more than 50 documents\n",
    "    categories = [c[0] for c in categories if c[1] > 50]\n",
    "    filtered = [i for i, lb in enumerate(labels) if lb in categories]\n",
    "    labels = [categories.index(labels[i]) for i in filtered]\n",
    "    test_docs = [test_docs[i] for i in filtered]\n",
    "    corpus.build(vocab_size=5000)\n",
    "    \n",
    "    print(\"3. Load GloVe embeddings.\")\n",
    "    embed_loader = EmbeddingLoader(size=ae.embed_size)\n",
    "    embedding = embed_loader.load(ae.seq_size, corpus)\n",
    "    \n",
    "    print(\"4. Use model's encoder to classify the documents.\")\n",
    "    from sklearn.cluster import KMeans\n",
    "    docs = corpus.docs_to_matrix(test_docs, ae.seq_size)\n",
    "    doc_vecs = embedding[docs]\n",
    "    # Use encoder from trained model\n",
    "    features = ae.get_encoder().predict(doc_vecs)\n",
    "    clf = KMeans(n_clusters=len(categories))\n",
    "    # Clustering by KMeans\n",
    "    clf.fit(features)\n",
    "    ae_dist = clf.inertia_\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    test_doc_words = [\" \".join(reuters.words(d)) for d in test_docs]\n",
    "    # scikit-learnのCountVectorizerで分のリストから単語カウントのベクトルを生成\n",
    "    vectorizer = CountVectorizer(vocabulary=corpus.vocab)\n",
    "    c_features = vectorizer.fit_transform(test_doc_words)\n",
    "    clf.fit(c_features)\n",
    "    cnt_dist = clf.inertia_\n",
    "    print(\" Sum of distances^2 of samples to their closest center is\")\n",
    "    print(\" Autoencoder: {}\".format(ae_dist))\n",
    "    print(\" Word count base: {}\".format(cnt_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Load the trained model.\n",
      "2. Prepare the corpus.\n",
      "3. Load GloVe embeddings.\n",
      "4. Use model's encoder to classify the documents.\n",
      " Sum of distances^2 of samples to their closest center is\n",
      " Autoencoder: 3211.137100514646\n",
      " Word count base: 286517.9755947776\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser( description=\"Try text autoencoder by reuters corpus\" )\n",
    "    parser.add_argument(\n",
    "        \"--predict\", action=\"store_const\", const=True, default=False,\n",
    "        help=\"Classify the sentences by trained model\"\n",
    "    )\n",
    "    args = parser.parse_args(args=[\"--predict\"])\n",
    "    log_dir = os.path.join(os.path.dirname(\"__file__\"), \"logs\")\n",
    "    if args.predict:\n",
    "        predict(log_dir)\n",
    "    else:\n",
    "        main(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
